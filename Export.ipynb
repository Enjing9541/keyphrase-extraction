{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swisscom_ai.research_keyphrase.embeddings.emb_distrib_local import EmbeddingDistributorLocal\n",
    "from swisscom_ai.research_keyphrase.model.input_representation import InputTextObj\n",
    "from swisscom_ai.research_keyphrase.model.method import MMRPhrase\n",
    "from swisscom_ai.research_keyphrase.preprocessing.postagging import PosTaggingStanford\n",
    "from swisscom_ai.research_keyphrase.util.fileIO import read_file\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pos tagger\n",
    "def load_local_pos_tagger(lang):\n",
    "    assert (lang in ['en', 'de', 'fr']), \"Only english 'en', german 'de' and french 'fr' are handled\"\n",
    "    #jar_path = config_parser.get('STANFORDTAGGER', 'jar_path')\n",
    "    stanford_ner_dir = './stanford-postagger-full-2018-10-16/'\n",
    "    model_directory_path= './stanford-postagger-full-2018-10-16/models/'\n",
    "    jar_path= stanford_ner_dir + 'stanford-postagger.jar'\n",
    "\n",
    "    return PosTaggingStanford(jar_path, model_directory_path, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance Similarity Calculation\n",
    "def _MMR(candidates, X, doc_embedd, beta, N):\n",
    "    \"\"\"\n",
    "    Core method using Maximal Marginal Relevance in charge to return the top-N candidates\n",
    "    :param embdistrib: embdistrib: embedding distributor see @EmbeddingDistributor\n",
    "    :param text_obj: Input text representation see @InputTextObj\n",
    "    :param candidates: list of candidates (string)\n",
    "    :param X: numpy array with the embedding of each candidate in each row\n",
    "    :param beta: hyperparameter beta for MMR (control tradeoff between informativeness and diversity)\n",
    "    :param N: number of candidates to extract\n",
    "    :param use_filtered: if true filter the text by keeping only candidate word before computing the doc embedding\n",
    "    :return: A tuple with 3 elements :\n",
    "    1)list of the top-N candidates (or less if there are not enough candidates) (list of string)\n",
    "    2)list of associated relevance scores (list of float)\n",
    "    3)list containing for each keyphrase a list of alias (list of list of string)\n",
    "    \"\"\"\n",
    "\n",
    "    N = min(N, len(candidates))\n",
    "    doc_sim = cosine_similarity(X, doc_embedd)\n",
    "\n",
    "    doc_sim_norm = doc_sim/np.max(doc_sim)\n",
    "    doc_sim_norm = 0.5 + (doc_sim_norm - np.average(doc_sim_norm)) / np.std(doc_sim_norm)\n",
    "\n",
    "    sim_between = cosine_similarity(X)\n",
    "    np.fill_diagonal(sim_between, np.NaN)\n",
    "\n",
    "    sim_between_norm = sim_between/np.nanmax(sim_between, axis=0)\n",
    "    sim_between_norm = \\\n",
    "        0.5 + (sim_between_norm - np.nanmean(sim_between_norm, axis=0)) / np.nanstd(sim_between_norm, axis=0)\n",
    "\n",
    "    selected_candidates = []\n",
    "    unselected_candidates = [c for c in range(len(candidates))]\n",
    "\n",
    "    j = np.argmax(doc_sim)\n",
    "    selected_candidates.append(j)\n",
    "    unselected_candidates.remove(j)\n",
    "\n",
    "    for _ in range(N - 1):\n",
    "        selec_array = np.array(selected_candidates)\n",
    "        unselec_array = np.array(unselected_candidates)\n",
    "\n",
    "        distance_to_doc = doc_sim_norm[unselec_array, :]\n",
    "        dist_between = sim_between_norm[unselec_array][:, selec_array]\n",
    "        if dist_between.ndim == 1:\n",
    "            dist_between = dist_between[:, np.newaxis]\n",
    "        j = np.argmax(beta * distance_to_doc - (1 - beta) * np.max(dist_between, axis=1).reshape(-1, 1))\n",
    "        item_idx = unselected_candidates[j]\n",
    "        selected_candidates.append(item_idx)\n",
    "        unselected_candidates.remove(item_idx)\n",
    "    return selected_candidates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48183614  0.09851979 -0.1985879  ... -0.5551104   0.41743463\n",
      "   0.22774616]\n",
      " [-0.2639316  -0.00578277 -1.0667024  ... -0.3865105  -0.10839391\n",
      "   0.10706019]]\n",
      "(2, 600)\n"
     ]
    }
   ],
   "source": [
    "import sent2vec\n",
    "\n",
    "#Load sent2vec model\n",
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('./sent2vec/wiki_unigrams.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  2007.abstr\n",
      "predicted keyphrase:  ['high-fidelity finite element model', 'finite element model', 'detailed finite element model', 'hexahedral finite element mesh', 'nurbs', 'nonuniform rational b-spline', 'hyperelastic material model', 'polygonal surfaces', 'organ reconstruction software package', 'polygonal representation', 'available meshing software', 'model', 'biological soft tissues', 'image segmentation', 'biomechanical research']\n",
      "['physically based animation', '2D VHF images', 'kidney', 'Visible Human Female project', 'software package', 'polygonal surfaces', 'biological soft tissues', 'high-fidelity finite element model', 'image segmentation', 'NURBS', 'viscoelastic model', 'biomechanical research', 'trauma research']\n",
      "Processing:  2042.abstr\n",
      "predicted keyphrase:  ['massless fluid', 'massless fluid representation', 'electromagnetic hybrid model', 'magnetopause shear layer', 'three-dimensional hybrid simulation', 'electrons', 'velocity shear region', 'magnetic field', 'space plasmas', 'tangential discontinuity', 'compressibility', 'hybrid simulation', 'plasma flow', 'cylindrical plasma source', 'particles']\n",
      "['tangential discontinuity', 'magnetized plasma flow', 'hybrid simulation', 'transition layer', 'magnetopause shear layer', 'field reversal layer', 'massless fluid representation', 'electromagnetic hybrid model', 'space plasmas', 'pressure anisotropy']\n",
      "0.4782608695652174\n",
      "0.36666666666666664\n",
      "0.41509433962264153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enjing9541/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/enjing9541/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: Mean of empty slice\n",
      "/Users/enjing9541/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1545: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from swisscom_ai.research_keyphrase.model.extractor import extract_candidates, extract_sent_candidates\n",
    "pos_tagger = load_local_pos_tagger('en')\n",
    "count = 0\n",
    "tp = 0\n",
    "r = 0\n",
    "p = 0\n",
    "F = 0\n",
    "for file in os.listdir(\"./Dataset/Hulth2003/Test/\"):\n",
    "    # select only 100 sample text to test\n",
    "    if(count == 2): break\n",
    "    if file.endswith(\".abstr\"):\n",
    "        path = os.path.join(\"./Dataset/Hulth2003/Test/\", file)\n",
    "        print('Processing: ',file)\n",
    "        f=open(path,'r').read()\n",
    "        f = f.replace(\"\\n\", \" \")\n",
    "        f = f.replace(\"\\t\", \" \")\n",
    "        raw_text = f\n",
    "        #print(raw_text)\n",
    "        tagged = pos_tagger.pos_tag_raw_text(raw_text)\n",
    "        text_obj = InputTextObj(tagged, 'en')\n",
    "        # List of candidates based on PosTag rules\n",
    "        candidates = np.array(extract_candidates(text_obj))  \n",
    "        # Remove Duplicates\n",
    "        candidates = list(set(candidates))\n",
    "        #print(candidates)\n",
    "        tagged = text_obj.filtered_pos_tagged\n",
    "        tokenized_doc_text = ' '.join(token[0].lower() for sent in tagged for token in sent)\n",
    "        #print(tokenized_doc_text)\n",
    "        document = tokenized_doc_text\n",
    "        #print(document)\n",
    "        doc = model.embed_sentence(document) \n",
    "        #print(doc)\n",
    "        #print(doc.shape)\n",
    "        doc = doc.reshape(1, -1)\n",
    "        #print(doc.shape)\n",
    "        score = []\n",
    "        for x in candidates:\n",
    "            sentence = x\n",
    "            res = model.embed_sentence(x) \n",
    "            score.append(res)\n",
    "        #print(score)\n",
    "        #print(score.shape)\n",
    "        score = np.asarray(score)\n",
    "        # result contains index of the top 15 candidate keyphrases\n",
    "        result = _MMR(candidates, score, doc, 1, 15)\n",
    "        #print(result)\n",
    "        pred_kp = []\n",
    "        for i in range(len(result)):\n",
    "            pred_kp.append(candidates[result[i]].lower())\n",
    "        print('predicted keyphrase: ',pred_kp)\n",
    "        f2=open(\"./Dataset/Hulth2003/Test/\" + file[:-6]+\".uncontr\",'r').read()\n",
    "        f2 = f2.replace(\"\\n\", \"\")\n",
    "        f2 = f2.replace(\"\\t\", \" \")\n",
    "        f2 = f2.split('; ')\n",
    "        actual_kp = f2\n",
    "        for x in actual_kp:\n",
    "            x_lower = x.lower()\n",
    "            if raw_text.find(x_lower) == -1:\n",
    "                actual_kp.remove(x)\n",
    "        for x in actual_kp:\n",
    "            x = x.lower()\n",
    "        actual_kp = list(set(actual_kp))\n",
    "        print(actual_kp)\n",
    "        intersect = list(set(pred_kp).intersection(actual_kp))\n",
    "        tp = tp + len(intersect)\n",
    "        #print(intersect)\n",
    "        #print(tp)\n",
    "        r = r + len(pred_kp)\n",
    "        #print(r)\n",
    "        p = p + len(actual_kp)\n",
    "        #print(p)\n",
    "        count = count + 1\n",
    "        \n",
    "precision = tp / p\n",
    "recall = tp / r\n",
    "F = 2*precision*recall / (precision + recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(F)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
